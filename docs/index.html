<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>sam_onnx API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sam_onnx</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from copy import deepcopy
from typing import Any, Tuple, Union, List
import cv2
import matplotlib.pyplot as plt
import numpy as np
import onnxruntime as ort
import glob
import gdown
import os

def check_and_download_weights(model_name=&#39;l0&#39;):

    __supported_modelnames = [&#39;l0&#39;, &#39;xl0&#39;]

    assert model_name in __supported_modelnames, f&#39;Model name not supported. Please use one of : {__supported_modelnames}&#39;
    
    l0_weights = {&#39;encoder&#39; : &#39;https://drive.google.com/file/d/1a0tRmHQeGTAbSeMqBMhu4DinsOR3cSv6/view?usp=sharing&#39;,
                 &#39;decoder&#39;: &#39;https://drive.google.com/file/d/13J7pNfh016sBqOQ17CludkUFdKgkkyQM/view?usp=sharing&#39;}
    
    xl0_weights = {&#39;encoder&#39;: &#39;https://drive.google.com/file/d/1NzavgCAqk6mSzTnQ_LKfl78V_O68lWNX/view?usp=sharing&#39;,
                  &#39;decoder&#39;: &#39;https://drive.google.com/file/d/1lrn5bQRE01Mwtp-nr9DBNTHcxk4Q6iiP/view?usp=sharing&#39;}
    
    if os.path.exists(&#39;model_weights&#39;):
        model_weights_folder_path = os.path.abspath(&#39;model_weights&#39;)
    else:
        os.makedirs(&#39;model_weights&#39;, 
                    exist_ok = False)
        model_weights_folder_path = os.path.abspath(&#39;model_weights&#39;)
    
    if os.path.exists(f&#39;model_weights/{model_name}/encoder.onnx&#39;):
        encoder_weights_path = os.path.abspath(f&#39;model_weights/{model_name}/encoder.onnx&#39;)
    else:
        os.makedirs(f&#39;model_weights/{model_name}&#39;, 
                    exist_ok = True)
        if model_name == &#39;l0&#39;:
            gdown.download(l0_weights[&#39;encoder&#39;],
                           f&#39;model_weights/{model_name}/encoder.onnx&#39;, 
                           fuzzy=True)
        if model_name == &#39;xl0&#39;:
            gdown.download(xl0_weights[&#39;encoder&#39;],
                           f&#39;model_weights/{model_name}/encoder.onnx&#39;, 
                           fuzzy=True)
        encoder_weights_path = os.path.abspath(f&#39;model_weights/{model_name}/encoder.onnx&#39;)
            
    if os.path.exists(f&#39;model_weights/{model_name}/decoder.onnx&#39;):
        decoder_weights_path = os.path.abspath(f&#39;model_weights/{model_name}/decoder.onnx&#39;)
    else:
        if model_name == &#39;l0&#39;:
            gdown.download(l0_weights[&#39;decoder&#39;],
                           f&#39;model_weights/{model_name}/decoder.onnx&#39;, 
                           fuzzy=True)
        if model_name == &#39;xl0&#39;:
            gdown.download(xl0_weights[&#39;decoder&#39;],
                           f&#39;model_weights/{model_name}/decoder.onnx&#39;, 
                           fuzzy=True)
        decoder_weights_path = os.path.abspath(f&#39;model_weights/{model_name}/decoder.onnx&#39;)

    return encoder_weights_path, decoder_weights_path
    
def show_mask(mask, ax, random_color=False):
    &#34;&#34;&#34;
    Visualize a mask image on the given axis.

    Parameters
    ----------
    mask : np.ndarray
        The mask image to visualize.
    ax : matplotlib.axes.Axes
        The axis to plot on.
    random_color : bool, optional
        Whether to use a random color for the mask, by default False
    &#34;&#34;&#34;
    if random_color:
        # Create a random color with some transparency
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        # Use a specific color with some transparency
        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)


def show_points(coords, labels, ax, marker_size=375):
    &#34;&#34;&#34;
    Show points on the axis.

    Parameters
    ----------
    coords : np.ndarray
        The coordinates of the points to show.
    labels : np.ndarray
        The labels of the points.
    ax : matplotlib.axes.Axes
        The axis to plot on.
    marker_size : int, optional
        The size of the markers, by default 375
    &#34;&#34;&#34;
    pos_points = coords[labels == 1]
    neg_points = coords[labels == 0]
    ax.scatter(
        pos_points[:, 0],
        pos_points[:, 1],
        color=&#34;green&#34;,
        marker=&#34;*&#34;,
        s=marker_size,
        edgecolor=&#34;white&#34;,
        linewidth=1.25,
    )
    ax.scatter(
        neg_points[:, 0],
        neg_points[:, 1],
        color=&#34;red&#34;,
        marker=&#34;*&#34;,
        s=marker_size,
        edgecolor=&#34;white&#34;,
        linewidth=1.25,
    )


def show_box(box, ax):
    &#34;&#34;&#34;
    Show a bounding box on the axis.

    Parameters
    ----------
    box : list
        The bounding box coordinates as [x0, y0, x1, y1].
    ax : matplotlib.axes.Axes
        The axis to plot on.
    &#34;&#34;&#34;
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(
        plt.Rectangle((x0, y0), w, h, edgecolor=&#34;green&#34;, facecolor=(0, 0, 0, 0), lw=2)
    )


class SamEncoder:
    &#34;&#34;&#34;
    The encoder class that loads and runs the SAM encoder model.

    Parameters
    ----------
    model_path: str
        The path to the encoder model.
    device: str, optional (default is &#39;cpu&#39;)
        The device to run the model, either &#39;cuda&#39; or &#39;cpu&#39;.
    kwargs: dict
        Additional arguments to be passed to the `InferenceSession` class from
        the onnxruntime library.

    Attributes
    ----------
    session: InferenceSession
        The loaded encoder model.
    input_name: str
        The name of the input layer of the model.
    &#34;&#34;&#34;

    def __init__(self, model_path: str, device: str = &#34;cpu&#34;, **kwargs):
        opt = ort.SessionOptions()

        if device == &#34;cuda&#34;:
            provider = [&#34;CUDAExecutionProvider&#34;]
        elif device == &#34;cpu&#34;:
            provider = [&#34;CPUExecutionProvider&#34;]
        else:
            raise ValueError(&#34;Invalid device, please use &#39;cuda&#39; or &#39;cpu&#39; device.&#34;)

        print(f&#34;loading encoder model from {model_path}...&#34;)
        self.session = ort.InferenceSession(
            model_path, opt, providers=provider, **kwargs
        )
        self.input_name = self.session.get_inputs()[0].name

    def _extract_feature(self, tensor: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Extract the feature from the input image tensor using the loaded
        encoder model.

        Parameters
        ----------
        tensor: numpy.ndarray
            The input image tensor.

        Returns
        -------
        feature: numpy.ndarray
            The feature extracted from the input image.
        &#34;&#34;&#34;
        feature = self.session.run(None, {self.input_name: tensor})[0]
        return feature

    def __call__(self, img: np.array, *args: Any, **kwds: Any) -&gt; Any:
        &#34;&#34;&#34;
        Call the encoder with the input image.

        Parameters
        ----------
        img: numpy.ndarray
            The input image.
        args, kwargs:
            Additional positional and keyword arguments to be passed to the
            encoder.

        Returns
        -------
        feature: numpy.ndarray
            The feature extracted from the input image.
        &#34;&#34;&#34;
        return self._extract_feature(img)


class SamDecoder:
    &#34;&#34;&#34;
    The decoder class that loads and runs the SAM decoder model.

    Parameters
    ----------
    model_path: str
        The path to the decoder model.
    device: str, default=&#34;cpu&#34;
        The device to run the model, either &#34;cuda&#34; or &#34;cpu&#34;.
    target_size: int, default=1024
        The target size of the output mask. The final mask size may be
        smaller if the original image is too small.
    mask_threshold: float, default=0.0
        The threshold value to binarize the output mask.
    kwargs: Any
        Additional arguments to be passed to onnxruntime.InferenceSession.

    Attributes
    ----------
    target_size: int
        The target size of the output mask.
    mask_threshold: float
        The threshold value to binarize the output mask.
    session: onnxruntime.InferenceSession
        The inference session of the loaded decoder model.
    &#34;&#34;&#34;

    def __init__(
        self,
        model_path: str,
        device: str = &#34;cpu&#34;,
        target_size: int = 1024,
        mask_threshold: float = 0.0,
        **kwargs,
    ):
        opt = ort.SessionOptions()

        if device == &#34;cuda&#34;:
            provider = [&#34;CUDAExecutionProvider&#34;]
        elif device == &#34;cpu&#34;:
            provider = [&#34;CPUExecutionProvider&#34;]
        else:
            raise ValueError(&#34;Invalid device, please use &#39;cuda&#39; or &#39;cpu&#39; device.&#34;)

        print(f&#34;loading decoder model from {model_path}...&#34;)
        self.target_size = target_size
        self.mask_threshold = mask_threshold
        self.session = ort.InferenceSession(
            model_path, opt, providers=provider, **kwargs
        )

    @staticmethod
    def get_preprocess_shape(
        oldh: int, oldw: int, long_side_length: int
    ) -&gt; Tuple[int, int]:
        &#34;&#34;&#34;
        Compute the output size given input size and target long side length.

        Parameters
        ----------
        oldh: int
            The height of the input image.
        oldw: int
            The width of the input image.
        long_side_length: int
            The target long side length of the output image.

        Returns
        -------
        Tuple[int, int]
            The (height, width) of the output image after resizing.
        &#34;&#34;&#34;
        scale = long_side_length * 1.0 / max(oldh, oldw)
        newh, neww = oldh * scale, oldw * scale
        neww = int(neww + 0.5)
        newh = int(newh + 0.5)
        return (newh, neww)

    def run(
        self,
        img_embeddings: np.ndarray,
        origin_image_size: Union[list, tuple],
        point_coords: Union[list, np.ndarray] = None,
        point_labels: Union[list, np.ndarray] = None,
        boxes: Union[list, np.ndarray] = None,
        return_logits: bool = False,
    ) -&gt; Tuple[np.ndarray, Any, Any]:
        &#34;&#34;&#34;
        Run the SAM decoder to segment an input image.

        Parameters
        ----------
        img_embeddings: np.ndarray
            The image embeddings obtained from SAM encoder.
            The shape should be (1, 256, 64, 64).
        origin_image_size: Union[list, tuple]
            The original size of the input image, (height, width)
        point_coords: Union[list, np.ndarray], optional
            The coordinates of the points in the input image.
            The shape should be (N, 2), where N is the number of points.
        point_labels: Union[list, np.ndarray], optional
            The labels of the points.
            The shape should be (N,) where N is the number of points.
        boxes: Union[list, np.ndarray], optional
            The coordinates of the bounding boxes in the input image.
            The shape should be (M, 4), where M is the number of boxes.
        return_logits: bool, default False
            Whether to return the logits (before sigmoid) of the mask predictions.

        Returns
        -------
        Tuple[np.ndarray, Any, Any]
            The segmentation masks, IoU scores, and low-resolution masks.
        &#34;&#34;&#34;
        input_size = self.get_preprocess_shape(
            *origin_image_size, long_side_length=self.target_size
        )

        if point_coords is None and point_labels is None and boxes is None:
            raise ValueError(
                &#34;Unable to segment, please input at least one box or point.&#34;
            )

        if img_embeddings.shape != (1, 256, 64, 64):
            raise ValueError(&#34;Got wrong embedding shape!&#34;)

        if point_coords is not None:
            point_coords = self.apply_coords(
                point_coords, origin_image_size, input_size
            ).astype(np.float32)

            prompts, labels = point_coords, point_labels

        if boxes is not None:
            boxes = self.apply_boxes(boxes, origin_image_size, input_size).astype(
                np.float32
            )
            box_labels = np.array(
                [[2, 3] for _ in range(boxes.shape[0])], dtype=np.float32
            ).reshape((-1, 2))

            if point_coords is not None:
                prompts = np.concatenate([prompts, boxes], axis=1)
                labels = np.concatenate([labels, box_labels], axis=1)
            else:
                prompts, labels = boxes, box_labels

        input_dict = {
            &#34;image_embeddings&#34;: img_embeddings,
            &#34;point_coords&#34;: prompts,
            &#34;point_labels&#34;: labels,
        }

        # Run the inference
        low_res_masks, iou_predictions = self.session.run(None, input_dict)

        # Post-process the masks
        masks = np_mask_postprocessing(low_res_masks, np.array(origin_image_size))

        if not return_logits:
            masks = masks &gt; self.mask_threshold

        return masks, iou_predictions, low_res_masks

    def apply_coords(self, coords, original_size, new_size):
        &#34;&#34;&#34;
        Applies the resizing to the coordinates.

        Parameters
        ----------
        coords : np.ndarray
            The coordinates to be resized.
            The shape should be (N, 2), where N is the number of points.
        original_size : Union[list, tuple]
            The original size of the input image, (height, width)
        new_size : Union[list, tuple]
            The new size of the input image, (height, width)

        Returns
        -------
        np.ndarray
            The resized coordinates.
        &#34;&#34;&#34;
        old_h, old_w = original_size
        new_h, new_w = new_size
        coords = deepcopy(coords).astype(float)
        coords[..., 0] = coords[..., 0] * (new_w / old_w)
        coords[..., 1] = coords[..., 1] * (new_h / old_h)
        return coords

    def apply_boxes(self, boxes, original_size, new_size):
        &#34;&#34;&#34;
        Applies the resizing to the bounding boxes.

        Parameters
        ----------
        boxes : np.ndarray
            The coordinates of the bounding boxes in the input image.
            The shape should be (M, 4), where M is the number of boxes.
        original_size : Union[list, tuple]
            The original size of the input image, (height, width)
        new_size : Union[list, tuple]
            The new size of the input image, (height, width)

        Returns
        -------
        np.ndarray
            The resized bounding boxes.
        &#34;&#34;&#34;
        boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size, new_size)
        return boxes


def np_resize_longest_image_size(
    input_image_size: np.array, longest_side: int
) -&gt; np.array:
    &#34;&#34;&#34;Resizes the image size to the longest side.

    Parameters
    ----------
    input_image_size : np.array
        Size of the input image in (height, width) format.
    longest_side : int
        Desired longest side of the resized image.

    Returns
    -------
    np.array
        Size of the resized image in (height, width) format.
    &#34;&#34;&#34;
    scale = longest_side / np.max(input_image_size)
    transformed_size = scale * input_image_size
    transformed_size = np.floor(transformed_size + 0.5).astype(np.int64)
    return transformed_size


def np_interp(x: np.array, size: tuple) -&gt; np.array:
    &#34;&#34;&#34;Interpolates a batch of masks to a given size.

    Parameters
    ----------
    x : np.array
        A batch of masks with shape (batch_size, 1, height, width).
    size : tuple
        Desired size of the masks (height, width) format.

    Returns
    -------
    np.array
        A batch of interpolated masks with shape (batch_size, 1, height, width).
    &#34;&#34;&#34;
    _rmsk = []
    for m in range(x.shape[0]):
        msk = x[m, 0, :, :]
        resized_array = cv2.resize(msk, size, interpolation=cv2.INTER_LINEAR)
        _rmsk.append(resized_array)
    np_rmsk = np.array(_rmsk)
    np_rmsk = np_rmsk[:, np.newaxis, :, :]
    return np_rmsk


def np_mask_postprocessing(masks: np.array, orig_im_size: np.array) -&gt; np.array:
    &#34;&#34;&#34;
    Perform postprocessing on predicted masks by interpolating them to
    desired size and then resizing them back to original image size.

    Parameters
    ----------
    masks : np.array
        Predicted masks.
    orig_im_size : np.array
        Original image size.

    Returns
    -------
    np.array
        Postprocessed masks.
    &#34;&#34;&#34;

    img_size = 1024  # Desired output size
    masks = np_interp(masks, (img_size, img_size))

    # Pad predicted masks to desired output size
    prepadded_size = np_resize_longest_image_size(orig_im_size, img_size)
    masks = masks[..., : int(prepadded_size[0]), : int(prepadded_size[1])]

    # Resize padded masks back to original image size
    origin_image_size = orig_im_size.astype(np.int64)
    w, h = origin_image_size[0], origin_image_size[1]
    masks = np_interp(masks, (h, w))
    return masks

def preprocess_np(x, img_size):
    &#34;&#34;&#34;
    Preprocess an image with mean and std normalization and padding to
    desired size.

    Parameters
    ----------
    x : numpy.ndarray
        Image to be preprocessed.
    img_size : int
        Desired size of the longer edge of the image.

    Returns
    -------
    numpy.ndarray
        Preprocessed image.

    &#34;&#34;&#34;
    pixel_mean = np.array([123.675 / 255, 116.28 / 255, 103.53 / 255]).astype(np.float32)
    pixel_std = np.array([58.395 / 255, 57.12 / 255, 57.375 / 255]).astype(np.float32)

    oh, ow, _ = x.shape
    long_side = max(oh, ow)
    if long_side != img_size:
        # Resize the image with long side == img_size
        scale = img_size * 1.0 / max(oh, ow)
        newh, neww = int(oh * scale + 0.5), int(ow * scale + 0.5)
        x = cv2.resize(x, (neww, newh))

    h, w = x.shape[:2]
    x = x.astype(np.float32) / 255  # Normalize to [0, 1]
    x = (x - pixel_mean) / pixel_std  # Normalize pixel values
    th, tw = img_size, img_size
    assert th &gt;= h and tw &gt;= w, &#34;image is too small&#34;

    # Pad the image with zeros if shorter than desired size
    x = np.pad(
        x,
        ((0, th - h), (0, tw - w), (0, 0)),
        mode=&#34;constant&#34;,
        constant_values=0,  # (top, bottom), (left, right)
    ).astype(np.float32)

    # Transpose the image from HWC to CHW and add batch dimension
    x = x.transpose((2, 0, 1))[np.newaxis, :, :, :]

    return x
            
class InferSAM:
    &#34;&#34;&#34;
    Class for inference with SAM models.

    Parameters
    ----------
    model_dir : str
        Directory containing trained SAM model.
    model_name : str, default &#39;l0&#39;
        Name of the model to use.
        Must be one of [&#39;l0&#39;, &#39;l1&#39;, &#39;l2&#39;, &#39;xl0&#39;, &#39;xl1&#39;].

    Attributes
    ----------
    model_name : str
        Name of the model to use.
    encoder : SamEncoder
        The encoder part of the SAM model.
    decoder : SamDecoder
        The decoder part of the SAM model.

    &#34;&#34;&#34;

    def __init__(self, model_name: str = &#34;l0&#34;):
        # assert model_dir is not None, &#34;model_dir is null&#34;
        assert model_name is not None, &#34;model_name is null&#34;

        self.model_name = model_name
        encoder_weights_path, decoder_weights_path = check_and_download_weights(model_name)
        
        # Find encoder and decoder models
        encoder_path = encoder_weights_path # glob.glob(model_dir + &#34;/*_encoder.onnx&#34;)[0]
        decoder_path = decoder_weights_path # glob.glob(model_dir + &#34;/*_decoder.onnx&#34;)[0]

        self.encoder = SamEncoder(encoder_path)
        self.decoder = SamDecoder(decoder_path)

        self.figsize = (10,10)

    def infer(
        self,
        img_path: str,
        boxes: List[list] = [[80, 50, 320, 420], [300, 20, 530, 420]],
        visualize=False,
    ) -&gt; np.array:
        &#34;&#34;&#34;
        Infer segmentation masks for a given image using the SAM model.

        Parameters
        ----------
        img_path : str
            Path to the input image.
        boxes : list of lists, default [[80, 50, 320, 420], [300, 20, 530, 420]]
            List of boxes, each box is a list of 4 ints, representing
            [xmax, ymax, xmin, ymin] coordinates.

        Returns
        -------
        masks : np.array
            A numpy array of shape (N, 1, H, W) containing segmentation masks,
            where N is the number of boxes, H and W are the height and width of
            the input image.

        &#34;&#34;&#34;
        assert img_path is not None, &#34;img_path is null&#34;

        raw_img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)
        assert raw_img is not None, &#34;raw_img is null&#34;

        origin_image_size = raw_img.shape[:2]

        img = None
        if self.model_name in [&#34;l0&#34;, &#34;l1&#34;, &#34;l2&#34;]:
            img = preprocess_np(raw_img, img_size=512)
        elif self.model_name in [&#34;xl0&#34;, &#34;xl1&#34;]:
            img = preprocess_np(raw_img, img_size=1024)
        assert img is not None, &#34;img is null&#34;

        boxes = np.array(boxes, dtype=np.float32)  # xmax, ymax, xmin, ymin

        img_embeddings = self.encoder(img)
        masks, _, _ = self.decoder.run(
            img_embeddings=img_embeddings,
            origin_image_size=origin_image_size,
            boxes=boxes,
        )
        if visualize:
            plt.figure(figsize=self.figsize)
            plt.imshow(raw_img)
            for mask in masks:
                show_mask(mask, plt.gca(), 
                          random_color=True)
            for box in boxes:
                show_box(box, plt.gca())
            plt.show()
        return masks

    def set_figsize(self,figsize=(10,10)):
        self.figsize = figsize</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sam_onnx.check_and_download_weights"><code class="name flex">
<span>def <span class="ident">check_and_download_weights</span></span>(<span>model_name='l0')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_and_download_weights(model_name=&#39;l0&#39;):

    __supported_modelnames = [&#39;l0&#39;, &#39;xl0&#39;]

    assert model_name in __supported_modelnames, f&#39;Model name not supported. Please use one of : {__supported_modelnames}&#39;
    
    l0_weights = {&#39;encoder&#39; : &#39;https://drive.google.com/file/d/1a0tRmHQeGTAbSeMqBMhu4DinsOR3cSv6/view?usp=sharing&#39;,
                 &#39;decoder&#39;: &#39;https://drive.google.com/file/d/13J7pNfh016sBqOQ17CludkUFdKgkkyQM/view?usp=sharing&#39;}
    
    xl0_weights = {&#39;encoder&#39;: &#39;https://drive.google.com/file/d/1NzavgCAqk6mSzTnQ_LKfl78V_O68lWNX/view?usp=sharing&#39;,
                  &#39;decoder&#39;: &#39;https://drive.google.com/file/d/1lrn5bQRE01Mwtp-nr9DBNTHcxk4Q6iiP/view?usp=sharing&#39;}
    
    if os.path.exists(&#39;model_weights&#39;):
        model_weights_folder_path = os.path.abspath(&#39;model_weights&#39;)
    else:
        os.makedirs(&#39;model_weights&#39;, 
                    exist_ok = False)
        model_weights_folder_path = os.path.abspath(&#39;model_weights&#39;)
    
    if os.path.exists(f&#39;model_weights/{model_name}/encoder.onnx&#39;):
        encoder_weights_path = os.path.abspath(f&#39;model_weights/{model_name}/encoder.onnx&#39;)
    else:
        os.makedirs(f&#39;model_weights/{model_name}&#39;, 
                    exist_ok = True)
        if model_name == &#39;l0&#39;:
            gdown.download(l0_weights[&#39;encoder&#39;],
                           f&#39;model_weights/{model_name}/encoder.onnx&#39;, 
                           fuzzy=True)
        if model_name == &#39;xl0&#39;:
            gdown.download(xl0_weights[&#39;encoder&#39;],
                           f&#39;model_weights/{model_name}/encoder.onnx&#39;, 
                           fuzzy=True)
        encoder_weights_path = os.path.abspath(f&#39;model_weights/{model_name}/encoder.onnx&#39;)
            
    if os.path.exists(f&#39;model_weights/{model_name}/decoder.onnx&#39;):
        decoder_weights_path = os.path.abspath(f&#39;model_weights/{model_name}/decoder.onnx&#39;)
    else:
        if model_name == &#39;l0&#39;:
            gdown.download(l0_weights[&#39;decoder&#39;],
                           f&#39;model_weights/{model_name}/decoder.onnx&#39;, 
                           fuzzy=True)
        if model_name == &#39;xl0&#39;:
            gdown.download(xl0_weights[&#39;decoder&#39;],
                           f&#39;model_weights/{model_name}/decoder.onnx&#39;, 
                           fuzzy=True)
        decoder_weights_path = os.path.abspath(f&#39;model_weights/{model_name}/decoder.onnx&#39;)

    return encoder_weights_path, decoder_weights_path</code></pre>
</details>
</dd>
<dt id="sam_onnx.np_interp"><code class="name flex">
<span>def <span class="ident">np_interp</span></span>(<span>x: <built-in function array>, size: tuple) ‑> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Interpolates a batch of masks to a given size.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.array</code></dt>
<dd>A batch of masks with shape (batch_size, 1, height, width).</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Desired size of the masks (height, width) format.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>A batch of interpolated masks with shape (batch_size, 1, height, width).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def np_interp(x: np.array, size: tuple) -&gt; np.array:
    &#34;&#34;&#34;Interpolates a batch of masks to a given size.

    Parameters
    ----------
    x : np.array
        A batch of masks with shape (batch_size, 1, height, width).
    size : tuple
        Desired size of the masks (height, width) format.

    Returns
    -------
    np.array
        A batch of interpolated masks with shape (batch_size, 1, height, width).
    &#34;&#34;&#34;
    _rmsk = []
    for m in range(x.shape[0]):
        msk = x[m, 0, :, :]
        resized_array = cv2.resize(msk, size, interpolation=cv2.INTER_LINEAR)
        _rmsk.append(resized_array)
    np_rmsk = np.array(_rmsk)
    np_rmsk = np_rmsk[:, np.newaxis, :, :]
    return np_rmsk</code></pre>
</details>
</dd>
<dt id="sam_onnx.np_mask_postprocessing"><code class="name flex">
<span>def <span class="ident">np_mask_postprocessing</span></span>(<span>masks: <built-in function array>, orig_im_size: <built-in function array>) ‑> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Perform postprocessing on predicted masks by interpolating them to
desired size and then resizing them back to original image size.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>masks</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Predicted masks.</dd>
<dt><strong><code>orig_im_size</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Original image size.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>Postprocessed masks.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def np_mask_postprocessing(masks: np.array, orig_im_size: np.array) -&gt; np.array:
    &#34;&#34;&#34;
    Perform postprocessing on predicted masks by interpolating them to
    desired size and then resizing them back to original image size.

    Parameters
    ----------
    masks : np.array
        Predicted masks.
    orig_im_size : np.array
        Original image size.

    Returns
    -------
    np.array
        Postprocessed masks.
    &#34;&#34;&#34;

    img_size = 1024  # Desired output size
    masks = np_interp(masks, (img_size, img_size))

    # Pad predicted masks to desired output size
    prepadded_size = np_resize_longest_image_size(orig_im_size, img_size)
    masks = masks[..., : int(prepadded_size[0]), : int(prepadded_size[1])]

    # Resize padded masks back to original image size
    origin_image_size = orig_im_size.astype(np.int64)
    w, h = origin_image_size[0], origin_image_size[1]
    masks = np_interp(masks, (h, w))
    return masks</code></pre>
</details>
</dd>
<dt id="sam_onnx.np_resize_longest_image_size"><code class="name flex">
<span>def <span class="ident">np_resize_longest_image_size</span></span>(<span>input_image_size: <built-in function array>, longest_side: int) ‑> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Resizes the image size to the longest side.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_image_size</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Size of the input image in (height, width) format.</dd>
<dt><strong><code>longest_side</code></strong> :&ensp;<code>int</code></dt>
<dd>Desired longest side of the resized image.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>Size of the resized image in (height, width) format.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def np_resize_longest_image_size(
    input_image_size: np.array, longest_side: int
) -&gt; np.array:
    &#34;&#34;&#34;Resizes the image size to the longest side.

    Parameters
    ----------
    input_image_size : np.array
        Size of the input image in (height, width) format.
    longest_side : int
        Desired longest side of the resized image.

    Returns
    -------
    np.array
        Size of the resized image in (height, width) format.
    &#34;&#34;&#34;
    scale = longest_side / np.max(input_image_size)
    transformed_size = scale * input_image_size
    transformed_size = np.floor(transformed_size + 0.5).astype(np.int64)
    return transformed_size</code></pre>
</details>
</dd>
<dt id="sam_onnx.preprocess_np"><code class="name flex">
<span>def <span class="ident">preprocess_np</span></span>(<span>x, img_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Preprocess an image with mean and std normalization and padding to
desired size.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Image to be preprocessed.</dd>
<dt><strong><code>img_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Desired size of the longer edge of the image.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Preprocessed image.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_np(x, img_size):
    &#34;&#34;&#34;
    Preprocess an image with mean and std normalization and padding to
    desired size.

    Parameters
    ----------
    x : numpy.ndarray
        Image to be preprocessed.
    img_size : int
        Desired size of the longer edge of the image.

    Returns
    -------
    numpy.ndarray
        Preprocessed image.

    &#34;&#34;&#34;
    pixel_mean = np.array([123.675 / 255, 116.28 / 255, 103.53 / 255]).astype(np.float32)
    pixel_std = np.array([58.395 / 255, 57.12 / 255, 57.375 / 255]).astype(np.float32)

    oh, ow, _ = x.shape
    long_side = max(oh, ow)
    if long_side != img_size:
        # Resize the image with long side == img_size
        scale = img_size * 1.0 / max(oh, ow)
        newh, neww = int(oh * scale + 0.5), int(ow * scale + 0.5)
        x = cv2.resize(x, (neww, newh))

    h, w = x.shape[:2]
    x = x.astype(np.float32) / 255  # Normalize to [0, 1]
    x = (x - pixel_mean) / pixel_std  # Normalize pixel values
    th, tw = img_size, img_size
    assert th &gt;= h and tw &gt;= w, &#34;image is too small&#34;

    # Pad the image with zeros if shorter than desired size
    x = np.pad(
        x,
        ((0, th - h), (0, tw - w), (0, 0)),
        mode=&#34;constant&#34;,
        constant_values=0,  # (top, bottom), (left, right)
    ).astype(np.float32)

    # Transpose the image from HWC to CHW and add batch dimension
    x = x.transpose((2, 0, 1))[np.newaxis, :, :, :]

    return x</code></pre>
</details>
</dd>
<dt id="sam_onnx.show_box"><code class="name flex">
<span>def <span class="ident">show_box</span></span>(<span>box, ax)</span>
</code></dt>
<dd>
<div class="desc"><p>Show a bounding box on the axis.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>box</code></strong> :&ensp;<code>list</code></dt>
<dd>The bounding box coordinates as [x0, y0, x1, y1].</dd>
<dt><strong><code>ax</code></strong> :&ensp;<code>matplotlib.axes.Axes</code></dt>
<dd>The axis to plot on.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_box(box, ax):
    &#34;&#34;&#34;
    Show a bounding box on the axis.

    Parameters
    ----------
    box : list
        The bounding box coordinates as [x0, y0, x1, y1].
    ax : matplotlib.axes.Axes
        The axis to plot on.
    &#34;&#34;&#34;
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(
        plt.Rectangle((x0, y0), w, h, edgecolor=&#34;green&#34;, facecolor=(0, 0, 0, 0), lw=2)
    )</code></pre>
</details>
</dd>
<dt id="sam_onnx.show_mask"><code class="name flex">
<span>def <span class="ident">show_mask</span></span>(<span>mask, ax, random_color=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Visualize a mask image on the given axis.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mask</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The mask image to visualize.</dd>
<dt><strong><code>ax</code></strong> :&ensp;<code>matplotlib.axes.Axes</code></dt>
<dd>The axis to plot on.</dd>
<dt><strong><code>random_color</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use a random color for the mask, by default False</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_mask(mask, ax, random_color=False):
    &#34;&#34;&#34;
    Visualize a mask image on the given axis.

    Parameters
    ----------
    mask : np.ndarray
        The mask image to visualize.
    ax : matplotlib.axes.Axes
        The axis to plot on.
    random_color : bool, optional
        Whether to use a random color for the mask, by default False
    &#34;&#34;&#34;
    if random_color:
        # Create a random color with some transparency
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        # Use a specific color with some transparency
        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)</code></pre>
</details>
</dd>
<dt id="sam_onnx.show_points"><code class="name flex">
<span>def <span class="ident">show_points</span></span>(<span>coords, labels, ax, marker_size=375)</span>
</code></dt>
<dd>
<div class="desc"><p>Show points on the axis.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>coords</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The coordinates of the points to show.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The labels of the points.</dd>
<dt><strong><code>ax</code></strong> :&ensp;<code>matplotlib.axes.Axes</code></dt>
<dd>The axis to plot on.</dd>
<dt><strong><code>marker_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The size of the markers, by default 375</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_points(coords, labels, ax, marker_size=375):
    &#34;&#34;&#34;
    Show points on the axis.

    Parameters
    ----------
    coords : np.ndarray
        The coordinates of the points to show.
    labels : np.ndarray
        The labels of the points.
    ax : matplotlib.axes.Axes
        The axis to plot on.
    marker_size : int, optional
        The size of the markers, by default 375
    &#34;&#34;&#34;
    pos_points = coords[labels == 1]
    neg_points = coords[labels == 0]
    ax.scatter(
        pos_points[:, 0],
        pos_points[:, 1],
        color=&#34;green&#34;,
        marker=&#34;*&#34;,
        s=marker_size,
        edgecolor=&#34;white&#34;,
        linewidth=1.25,
    )
    ax.scatter(
        neg_points[:, 0],
        neg_points[:, 1],
        color=&#34;red&#34;,
        marker=&#34;*&#34;,
        s=marker_size,
        edgecolor=&#34;white&#34;,
        linewidth=1.25,
    )</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sam_onnx.InferSAM"><code class="flex name class">
<span>class <span class="ident">InferSAM</span></span>
<span>(</span><span>model_name: str = 'l0')</span>
</code></dt>
<dd>
<div class="desc"><p>Class for inference with SAM models.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Directory containing trained SAM model.</dd>
<dt><strong><code>model_name</code></strong> :&ensp;<code>str</code>, default <code>'l0'</code></dt>
<dd>Name of the model to use.
Must be one of ['l0', 'l1', 'l2', 'xl0', 'xl1'].</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the model to use.</dd>
<dt><strong><code>encoder</code></strong> :&ensp;<code><a title="sam_onnx.SamEncoder" href="#sam_onnx.SamEncoder">SamEncoder</a></code></dt>
<dd>The encoder part of the SAM model.</dd>
<dt><strong><code>decoder</code></strong> :&ensp;<code><a title="sam_onnx.SamDecoder" href="#sam_onnx.SamDecoder">SamDecoder</a></code></dt>
<dd>The decoder part of the SAM model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InferSAM:
    &#34;&#34;&#34;
    Class for inference with SAM models.

    Parameters
    ----------
    model_dir : str
        Directory containing trained SAM model.
    model_name : str, default &#39;l0&#39;
        Name of the model to use.
        Must be one of [&#39;l0&#39;, &#39;l1&#39;, &#39;l2&#39;, &#39;xl0&#39;, &#39;xl1&#39;].

    Attributes
    ----------
    model_name : str
        Name of the model to use.
    encoder : SamEncoder
        The encoder part of the SAM model.
    decoder : SamDecoder
        The decoder part of the SAM model.

    &#34;&#34;&#34;

    def __init__(self, model_name: str = &#34;l0&#34;):
        # assert model_dir is not None, &#34;model_dir is null&#34;
        assert model_name is not None, &#34;model_name is null&#34;

        self.model_name = model_name
        encoder_weights_path, decoder_weights_path = check_and_download_weights(model_name)
        
        # Find encoder and decoder models
        encoder_path = encoder_weights_path # glob.glob(model_dir + &#34;/*_encoder.onnx&#34;)[0]
        decoder_path = decoder_weights_path # glob.glob(model_dir + &#34;/*_decoder.onnx&#34;)[0]

        self.encoder = SamEncoder(encoder_path)
        self.decoder = SamDecoder(decoder_path)

        self.figsize = (10,10)

    def infer(
        self,
        img_path: str,
        boxes: List[list] = [[80, 50, 320, 420], [300, 20, 530, 420]],
        visualize=False,
    ) -&gt; np.array:
        &#34;&#34;&#34;
        Infer segmentation masks for a given image using the SAM model.

        Parameters
        ----------
        img_path : str
            Path to the input image.
        boxes : list of lists, default [[80, 50, 320, 420], [300, 20, 530, 420]]
            List of boxes, each box is a list of 4 ints, representing
            [xmax, ymax, xmin, ymin] coordinates.

        Returns
        -------
        masks : np.array
            A numpy array of shape (N, 1, H, W) containing segmentation masks,
            where N is the number of boxes, H and W are the height and width of
            the input image.

        &#34;&#34;&#34;
        assert img_path is not None, &#34;img_path is null&#34;

        raw_img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)
        assert raw_img is not None, &#34;raw_img is null&#34;

        origin_image_size = raw_img.shape[:2]

        img = None
        if self.model_name in [&#34;l0&#34;, &#34;l1&#34;, &#34;l2&#34;]:
            img = preprocess_np(raw_img, img_size=512)
        elif self.model_name in [&#34;xl0&#34;, &#34;xl1&#34;]:
            img = preprocess_np(raw_img, img_size=1024)
        assert img is not None, &#34;img is null&#34;

        boxes = np.array(boxes, dtype=np.float32)  # xmax, ymax, xmin, ymin

        img_embeddings = self.encoder(img)
        masks, _, _ = self.decoder.run(
            img_embeddings=img_embeddings,
            origin_image_size=origin_image_size,
            boxes=boxes,
        )
        if visualize:
            plt.figure(figsize=self.figsize)
            plt.imshow(raw_img)
            for mask in masks:
                show_mask(mask, plt.gca(), 
                          random_color=True)
            for box in boxes:
                show_box(box, plt.gca())
            plt.show()
        return masks

    def set_figsize(self,figsize=(10,10)):
        self.figsize = figsize</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="sam_onnx.InferSAM.infer"><code class="name flex">
<span>def <span class="ident">infer</span></span>(<span>self, img_path: str, boxes: List[list] = [[80, 50, 320, 420], [300, 20, 530, 420]], visualize=False) ‑> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Infer segmentation masks for a given image using the SAM model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the input image.</dd>
<dt><strong><code>boxes</code></strong> :&ensp;<code>list</code> of <code>lists</code>, default <code>[[80, 50, 320, 420], [300, 20, 530, 420]]</code></dt>
<dd>List of boxes, each box is a list of 4 ints, representing
[xmax, ymax, xmin, ymin] coordinates.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>masks</code></strong> :&ensp;<code>np.array</code></dt>
<dd>A numpy array of shape (N, 1, H, W) containing segmentation masks,
where N is the number of boxes, H and W are the height and width of
the input image.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def infer(
    self,
    img_path: str,
    boxes: List[list] = [[80, 50, 320, 420], [300, 20, 530, 420]],
    visualize=False,
) -&gt; np.array:
    &#34;&#34;&#34;
    Infer segmentation masks for a given image using the SAM model.

    Parameters
    ----------
    img_path : str
        Path to the input image.
    boxes : list of lists, default [[80, 50, 320, 420], [300, 20, 530, 420]]
        List of boxes, each box is a list of 4 ints, representing
        [xmax, ymax, xmin, ymin] coordinates.

    Returns
    -------
    masks : np.array
        A numpy array of shape (N, 1, H, W) containing segmentation masks,
        where N is the number of boxes, H and W are the height and width of
        the input image.

    &#34;&#34;&#34;
    assert img_path is not None, &#34;img_path is null&#34;

    raw_img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)
    assert raw_img is not None, &#34;raw_img is null&#34;

    origin_image_size = raw_img.shape[:2]

    img = None
    if self.model_name in [&#34;l0&#34;, &#34;l1&#34;, &#34;l2&#34;]:
        img = preprocess_np(raw_img, img_size=512)
    elif self.model_name in [&#34;xl0&#34;, &#34;xl1&#34;]:
        img = preprocess_np(raw_img, img_size=1024)
    assert img is not None, &#34;img is null&#34;

    boxes = np.array(boxes, dtype=np.float32)  # xmax, ymax, xmin, ymin

    img_embeddings = self.encoder(img)
    masks, _, _ = self.decoder.run(
        img_embeddings=img_embeddings,
        origin_image_size=origin_image_size,
        boxes=boxes,
    )
    if visualize:
        plt.figure(figsize=self.figsize)
        plt.imshow(raw_img)
        for mask in masks:
            show_mask(mask, plt.gca(), 
                      random_color=True)
        for box in boxes:
            show_box(box, plt.gca())
        plt.show()
    return masks</code></pre>
</details>
</dd>
<dt id="sam_onnx.InferSAM.set_figsize"><code class="name flex">
<span>def <span class="ident">set_figsize</span></span>(<span>self, figsize=(10, 10))</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_figsize(self,figsize=(10,10)):
    self.figsize = figsize</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="sam_onnx.SamDecoder"><code class="flex name class">
<span>class <span class="ident">SamDecoder</span></span>
<span>(</span><span>model_path: str, device: str = 'cpu', target_size: int = 1024, mask_threshold: float = 0.0, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>The decoder class that loads and runs the SAM decoder model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the decoder model.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code>, default=<code>"cpu"</code></dt>
<dd>The device to run the model, either "cuda" or "cpu".</dd>
<dt><strong><code>target_size</code></strong> :&ensp;<code>int</code>, default=<code>1024</code></dt>
<dd>The target size of the output mask. The final mask size may be
smaller if the original image is too small.</dd>
<dt><strong><code>mask_threshold</code></strong> :&ensp;<code>float</code>, default=<code>0.0</code></dt>
<dd>The threshold value to binarize the output mask.</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>Any</code></dt>
<dd>Additional arguments to be passed to onnxruntime.InferenceSession.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>target_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The target size of the output mask.</dd>
<dt><strong><code>mask_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>The threshold value to binarize the output mask.</dd>
<dt><strong><code>session</code></strong> :&ensp;<code>onnxruntime.InferenceSession</code></dt>
<dd>The inference session of the loaded decoder model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SamDecoder:
    &#34;&#34;&#34;
    The decoder class that loads and runs the SAM decoder model.

    Parameters
    ----------
    model_path: str
        The path to the decoder model.
    device: str, default=&#34;cpu&#34;
        The device to run the model, either &#34;cuda&#34; or &#34;cpu&#34;.
    target_size: int, default=1024
        The target size of the output mask. The final mask size may be
        smaller if the original image is too small.
    mask_threshold: float, default=0.0
        The threshold value to binarize the output mask.
    kwargs: Any
        Additional arguments to be passed to onnxruntime.InferenceSession.

    Attributes
    ----------
    target_size: int
        The target size of the output mask.
    mask_threshold: float
        The threshold value to binarize the output mask.
    session: onnxruntime.InferenceSession
        The inference session of the loaded decoder model.
    &#34;&#34;&#34;

    def __init__(
        self,
        model_path: str,
        device: str = &#34;cpu&#34;,
        target_size: int = 1024,
        mask_threshold: float = 0.0,
        **kwargs,
    ):
        opt = ort.SessionOptions()

        if device == &#34;cuda&#34;:
            provider = [&#34;CUDAExecutionProvider&#34;]
        elif device == &#34;cpu&#34;:
            provider = [&#34;CPUExecutionProvider&#34;]
        else:
            raise ValueError(&#34;Invalid device, please use &#39;cuda&#39; or &#39;cpu&#39; device.&#34;)

        print(f&#34;loading decoder model from {model_path}...&#34;)
        self.target_size = target_size
        self.mask_threshold = mask_threshold
        self.session = ort.InferenceSession(
            model_path, opt, providers=provider, **kwargs
        )

    @staticmethod
    def get_preprocess_shape(
        oldh: int, oldw: int, long_side_length: int
    ) -&gt; Tuple[int, int]:
        &#34;&#34;&#34;
        Compute the output size given input size and target long side length.

        Parameters
        ----------
        oldh: int
            The height of the input image.
        oldw: int
            The width of the input image.
        long_side_length: int
            The target long side length of the output image.

        Returns
        -------
        Tuple[int, int]
            The (height, width) of the output image after resizing.
        &#34;&#34;&#34;
        scale = long_side_length * 1.0 / max(oldh, oldw)
        newh, neww = oldh * scale, oldw * scale
        neww = int(neww + 0.5)
        newh = int(newh + 0.5)
        return (newh, neww)

    def run(
        self,
        img_embeddings: np.ndarray,
        origin_image_size: Union[list, tuple],
        point_coords: Union[list, np.ndarray] = None,
        point_labels: Union[list, np.ndarray] = None,
        boxes: Union[list, np.ndarray] = None,
        return_logits: bool = False,
    ) -&gt; Tuple[np.ndarray, Any, Any]:
        &#34;&#34;&#34;
        Run the SAM decoder to segment an input image.

        Parameters
        ----------
        img_embeddings: np.ndarray
            The image embeddings obtained from SAM encoder.
            The shape should be (1, 256, 64, 64).
        origin_image_size: Union[list, tuple]
            The original size of the input image, (height, width)
        point_coords: Union[list, np.ndarray], optional
            The coordinates of the points in the input image.
            The shape should be (N, 2), where N is the number of points.
        point_labels: Union[list, np.ndarray], optional
            The labels of the points.
            The shape should be (N,) where N is the number of points.
        boxes: Union[list, np.ndarray], optional
            The coordinates of the bounding boxes in the input image.
            The shape should be (M, 4), where M is the number of boxes.
        return_logits: bool, default False
            Whether to return the logits (before sigmoid) of the mask predictions.

        Returns
        -------
        Tuple[np.ndarray, Any, Any]
            The segmentation masks, IoU scores, and low-resolution masks.
        &#34;&#34;&#34;
        input_size = self.get_preprocess_shape(
            *origin_image_size, long_side_length=self.target_size
        )

        if point_coords is None and point_labels is None and boxes is None:
            raise ValueError(
                &#34;Unable to segment, please input at least one box or point.&#34;
            )

        if img_embeddings.shape != (1, 256, 64, 64):
            raise ValueError(&#34;Got wrong embedding shape!&#34;)

        if point_coords is not None:
            point_coords = self.apply_coords(
                point_coords, origin_image_size, input_size
            ).astype(np.float32)

            prompts, labels = point_coords, point_labels

        if boxes is not None:
            boxes = self.apply_boxes(boxes, origin_image_size, input_size).astype(
                np.float32
            )
            box_labels = np.array(
                [[2, 3] for _ in range(boxes.shape[0])], dtype=np.float32
            ).reshape((-1, 2))

            if point_coords is not None:
                prompts = np.concatenate([prompts, boxes], axis=1)
                labels = np.concatenate([labels, box_labels], axis=1)
            else:
                prompts, labels = boxes, box_labels

        input_dict = {
            &#34;image_embeddings&#34;: img_embeddings,
            &#34;point_coords&#34;: prompts,
            &#34;point_labels&#34;: labels,
        }

        # Run the inference
        low_res_masks, iou_predictions = self.session.run(None, input_dict)

        # Post-process the masks
        masks = np_mask_postprocessing(low_res_masks, np.array(origin_image_size))

        if not return_logits:
            masks = masks &gt; self.mask_threshold

        return masks, iou_predictions, low_res_masks

    def apply_coords(self, coords, original_size, new_size):
        &#34;&#34;&#34;
        Applies the resizing to the coordinates.

        Parameters
        ----------
        coords : np.ndarray
            The coordinates to be resized.
            The shape should be (N, 2), where N is the number of points.
        original_size : Union[list, tuple]
            The original size of the input image, (height, width)
        new_size : Union[list, tuple]
            The new size of the input image, (height, width)

        Returns
        -------
        np.ndarray
            The resized coordinates.
        &#34;&#34;&#34;
        old_h, old_w = original_size
        new_h, new_w = new_size
        coords = deepcopy(coords).astype(float)
        coords[..., 0] = coords[..., 0] * (new_w / old_w)
        coords[..., 1] = coords[..., 1] * (new_h / old_h)
        return coords

    def apply_boxes(self, boxes, original_size, new_size):
        &#34;&#34;&#34;
        Applies the resizing to the bounding boxes.

        Parameters
        ----------
        boxes : np.ndarray
            The coordinates of the bounding boxes in the input image.
            The shape should be (M, 4), where M is the number of boxes.
        original_size : Union[list, tuple]
            The original size of the input image, (height, width)
        new_size : Union[list, tuple]
            The new size of the input image, (height, width)

        Returns
        -------
        np.ndarray
            The resized bounding boxes.
        &#34;&#34;&#34;
        boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size, new_size)
        return boxes</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="sam_onnx.SamDecoder.get_preprocess_shape"><code class="name flex">
<span>def <span class="ident">get_preprocess_shape</span></span>(<span>oldh: int, oldw: int, long_side_length: int) ‑> Tuple[int, int]</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the output size given input size and target long side length.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>oldh</code></strong> :&ensp;<code>int</code></dt>
<dd>The height of the input image.</dd>
<dt><strong><code>oldw</code></strong> :&ensp;<code>int</code></dt>
<dd>The width of the input image.</dd>
<dt><strong><code>long_side_length</code></strong> :&ensp;<code>int</code></dt>
<dd>The target long side length of the output image.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[int, int]</code></dt>
<dd>The (height, width) of the output image after resizing.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_preprocess_shape(
    oldh: int, oldw: int, long_side_length: int
) -&gt; Tuple[int, int]:
    &#34;&#34;&#34;
    Compute the output size given input size and target long side length.

    Parameters
    ----------
    oldh: int
        The height of the input image.
    oldw: int
        The width of the input image.
    long_side_length: int
        The target long side length of the output image.

    Returns
    -------
    Tuple[int, int]
        The (height, width) of the output image after resizing.
    &#34;&#34;&#34;
    scale = long_side_length * 1.0 / max(oldh, oldw)
    newh, neww = oldh * scale, oldw * scale
    neww = int(neww + 0.5)
    newh = int(newh + 0.5)
    return (newh, neww)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sam_onnx.SamDecoder.apply_boxes"><code class="name flex">
<span>def <span class="ident">apply_boxes</span></span>(<span>self, boxes, original_size, new_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies the resizing to the bounding boxes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>boxes</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The coordinates of the bounding boxes in the input image.
The shape should be (M, 4), where M is the number of boxes.</dd>
<dt><strong><code>original_size</code></strong> :&ensp;<code>Union[list, tuple]</code></dt>
<dd>The original size of the input image, (height, width)</dd>
<dt><strong><code>new_size</code></strong> :&ensp;<code>Union[list, tuple]</code></dt>
<dd>The new size of the input image, (height, width)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>The resized bounding boxes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_boxes(self, boxes, original_size, new_size):
    &#34;&#34;&#34;
    Applies the resizing to the bounding boxes.

    Parameters
    ----------
    boxes : np.ndarray
        The coordinates of the bounding boxes in the input image.
        The shape should be (M, 4), where M is the number of boxes.
    original_size : Union[list, tuple]
        The original size of the input image, (height, width)
    new_size : Union[list, tuple]
        The new size of the input image, (height, width)

    Returns
    -------
    np.ndarray
        The resized bounding boxes.
    &#34;&#34;&#34;
    boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size, new_size)
    return boxes</code></pre>
</details>
</dd>
<dt id="sam_onnx.SamDecoder.apply_coords"><code class="name flex">
<span>def <span class="ident">apply_coords</span></span>(<span>self, coords, original_size, new_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies the resizing to the coordinates.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>coords</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The coordinates to be resized.
The shape should be (N, 2), where N is the number of points.</dd>
<dt><strong><code>original_size</code></strong> :&ensp;<code>Union[list, tuple]</code></dt>
<dd>The original size of the input image, (height, width)</dd>
<dt><strong><code>new_size</code></strong> :&ensp;<code>Union[list, tuple]</code></dt>
<dd>The new size of the input image, (height, width)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>The resized coordinates.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_coords(self, coords, original_size, new_size):
    &#34;&#34;&#34;
    Applies the resizing to the coordinates.

    Parameters
    ----------
    coords : np.ndarray
        The coordinates to be resized.
        The shape should be (N, 2), where N is the number of points.
    original_size : Union[list, tuple]
        The original size of the input image, (height, width)
    new_size : Union[list, tuple]
        The new size of the input image, (height, width)

    Returns
    -------
    np.ndarray
        The resized coordinates.
    &#34;&#34;&#34;
    old_h, old_w = original_size
    new_h, new_w = new_size
    coords = deepcopy(coords).astype(float)
    coords[..., 0] = coords[..., 0] * (new_w / old_w)
    coords[..., 1] = coords[..., 1] * (new_h / old_h)
    return coords</code></pre>
</details>
</dd>
<dt id="sam_onnx.SamDecoder.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, img_embeddings: numpy.ndarray, origin_image_size: Union[list, tuple], point_coords: Union[list, numpy.ndarray] = None, point_labels: Union[list, numpy.ndarray] = None, boxes: Union[list, numpy.ndarray] = None, return_logits: bool = False) ‑> Tuple[numpy.ndarray, Any, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Run the SAM decoder to segment an input image.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img_embeddings</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The image embeddings obtained from SAM encoder.
The shape should be (1, 256, 64, 64).</dd>
<dt><strong><code>origin_image_size</code></strong> :&ensp;<code>Union[list, tuple]</code></dt>
<dd>The original size of the input image, (height, width)</dd>
<dt><strong><code>point_coords</code></strong> :&ensp;<code>Union[list, np.ndarray]</code>, optional</dt>
<dd>The coordinates of the points in the input image.
The shape should be (N, 2), where N is the number of points.</dd>
<dt><strong><code>point_labels</code></strong> :&ensp;<code>Union[list, np.ndarray]</code>, optional</dt>
<dd>The labels of the points.
The shape should be (N,) where N is the number of points.</dd>
<dt><strong><code>boxes</code></strong> :&ensp;<code>Union[list, np.ndarray]</code>, optional</dt>
<dd>The coordinates of the bounding boxes in the input image.
The shape should be (M, 4), where M is the number of boxes.</dd>
<dt><strong><code>return_logits</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>Whether to return the logits (before sigmoid) of the mask predictions.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[np.ndarray, Any, Any]</code></dt>
<dd>The segmentation masks, IoU scores, and low-resolution masks.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(
    self,
    img_embeddings: np.ndarray,
    origin_image_size: Union[list, tuple],
    point_coords: Union[list, np.ndarray] = None,
    point_labels: Union[list, np.ndarray] = None,
    boxes: Union[list, np.ndarray] = None,
    return_logits: bool = False,
) -&gt; Tuple[np.ndarray, Any, Any]:
    &#34;&#34;&#34;
    Run the SAM decoder to segment an input image.

    Parameters
    ----------
    img_embeddings: np.ndarray
        The image embeddings obtained from SAM encoder.
        The shape should be (1, 256, 64, 64).
    origin_image_size: Union[list, tuple]
        The original size of the input image, (height, width)
    point_coords: Union[list, np.ndarray], optional
        The coordinates of the points in the input image.
        The shape should be (N, 2), where N is the number of points.
    point_labels: Union[list, np.ndarray], optional
        The labels of the points.
        The shape should be (N,) where N is the number of points.
    boxes: Union[list, np.ndarray], optional
        The coordinates of the bounding boxes in the input image.
        The shape should be (M, 4), where M is the number of boxes.
    return_logits: bool, default False
        Whether to return the logits (before sigmoid) of the mask predictions.

    Returns
    -------
    Tuple[np.ndarray, Any, Any]
        The segmentation masks, IoU scores, and low-resolution masks.
    &#34;&#34;&#34;
    input_size = self.get_preprocess_shape(
        *origin_image_size, long_side_length=self.target_size
    )

    if point_coords is None and point_labels is None and boxes is None:
        raise ValueError(
            &#34;Unable to segment, please input at least one box or point.&#34;
        )

    if img_embeddings.shape != (1, 256, 64, 64):
        raise ValueError(&#34;Got wrong embedding shape!&#34;)

    if point_coords is not None:
        point_coords = self.apply_coords(
            point_coords, origin_image_size, input_size
        ).astype(np.float32)

        prompts, labels = point_coords, point_labels

    if boxes is not None:
        boxes = self.apply_boxes(boxes, origin_image_size, input_size).astype(
            np.float32
        )
        box_labels = np.array(
            [[2, 3] for _ in range(boxes.shape[0])], dtype=np.float32
        ).reshape((-1, 2))

        if point_coords is not None:
            prompts = np.concatenate([prompts, boxes], axis=1)
            labels = np.concatenate([labels, box_labels], axis=1)
        else:
            prompts, labels = boxes, box_labels

    input_dict = {
        &#34;image_embeddings&#34;: img_embeddings,
        &#34;point_coords&#34;: prompts,
        &#34;point_labels&#34;: labels,
    }

    # Run the inference
    low_res_masks, iou_predictions = self.session.run(None, input_dict)

    # Post-process the masks
    masks = np_mask_postprocessing(low_res_masks, np.array(origin_image_size))

    if not return_logits:
        masks = masks &gt; self.mask_threshold

    return masks, iou_predictions, low_res_masks</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="sam_onnx.SamEncoder"><code class="flex name class">
<span>class <span class="ident">SamEncoder</span></span>
<span>(</span><span>model_path: str, device: str = 'cpu', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>The encoder class that loads and runs the SAM encoder model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the encoder model.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code>, optional <code>(default is 'cpu')</code></dt>
<dd>The device to run the model, either 'cuda' or 'cpu'.</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Additional arguments to be passed to the <code>InferenceSession</code> class from
the onnxruntime library.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>session</code></strong> :&ensp;<code>InferenceSession</code></dt>
<dd>The loaded encoder model.</dd>
<dt><strong><code>input_name</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the input layer of the model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SamEncoder:
    &#34;&#34;&#34;
    The encoder class that loads and runs the SAM encoder model.

    Parameters
    ----------
    model_path: str
        The path to the encoder model.
    device: str, optional (default is &#39;cpu&#39;)
        The device to run the model, either &#39;cuda&#39; or &#39;cpu&#39;.
    kwargs: dict
        Additional arguments to be passed to the `InferenceSession` class from
        the onnxruntime library.

    Attributes
    ----------
    session: InferenceSession
        The loaded encoder model.
    input_name: str
        The name of the input layer of the model.
    &#34;&#34;&#34;

    def __init__(self, model_path: str, device: str = &#34;cpu&#34;, **kwargs):
        opt = ort.SessionOptions()

        if device == &#34;cuda&#34;:
            provider = [&#34;CUDAExecutionProvider&#34;]
        elif device == &#34;cpu&#34;:
            provider = [&#34;CPUExecutionProvider&#34;]
        else:
            raise ValueError(&#34;Invalid device, please use &#39;cuda&#39; or &#39;cpu&#39; device.&#34;)

        print(f&#34;loading encoder model from {model_path}...&#34;)
        self.session = ort.InferenceSession(
            model_path, opt, providers=provider, **kwargs
        )
        self.input_name = self.session.get_inputs()[0].name

    def _extract_feature(self, tensor: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Extract the feature from the input image tensor using the loaded
        encoder model.

        Parameters
        ----------
        tensor: numpy.ndarray
            The input image tensor.

        Returns
        -------
        feature: numpy.ndarray
            The feature extracted from the input image.
        &#34;&#34;&#34;
        feature = self.session.run(None, {self.input_name: tensor})[0]
        return feature

    def __call__(self, img: np.array, *args: Any, **kwds: Any) -&gt; Any:
        &#34;&#34;&#34;
        Call the encoder with the input image.

        Parameters
        ----------
        img: numpy.ndarray
            The input image.
        args, kwargs:
            Additional positional and keyword arguments to be passed to the
            encoder.

        Returns
        -------
        feature: numpy.ndarray
            The feature extracted from the input image.
        &#34;&#34;&#34;
        return self._extract_feature(img)</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sam_onnx.check_and_download_weights" href="#sam_onnx.check_and_download_weights">check_and_download_weights</a></code></li>
<li><code><a title="sam_onnx.np_interp" href="#sam_onnx.np_interp">np_interp</a></code></li>
<li><code><a title="sam_onnx.np_mask_postprocessing" href="#sam_onnx.np_mask_postprocessing">np_mask_postprocessing</a></code></li>
<li><code><a title="sam_onnx.np_resize_longest_image_size" href="#sam_onnx.np_resize_longest_image_size">np_resize_longest_image_size</a></code></li>
<li><code><a title="sam_onnx.preprocess_np" href="#sam_onnx.preprocess_np">preprocess_np</a></code></li>
<li><code><a title="sam_onnx.show_box" href="#sam_onnx.show_box">show_box</a></code></li>
<li><code><a title="sam_onnx.show_mask" href="#sam_onnx.show_mask">show_mask</a></code></li>
<li><code><a title="sam_onnx.show_points" href="#sam_onnx.show_points">show_points</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sam_onnx.InferSAM" href="#sam_onnx.InferSAM">InferSAM</a></code></h4>
<ul class="">
<li><code><a title="sam_onnx.InferSAM.infer" href="#sam_onnx.InferSAM.infer">infer</a></code></li>
<li><code><a title="sam_onnx.InferSAM.set_figsize" href="#sam_onnx.InferSAM.set_figsize">set_figsize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sam_onnx.SamDecoder" href="#sam_onnx.SamDecoder">SamDecoder</a></code></h4>
<ul class="">
<li><code><a title="sam_onnx.SamDecoder.apply_boxes" href="#sam_onnx.SamDecoder.apply_boxes">apply_boxes</a></code></li>
<li><code><a title="sam_onnx.SamDecoder.apply_coords" href="#sam_onnx.SamDecoder.apply_coords">apply_coords</a></code></li>
<li><code><a title="sam_onnx.SamDecoder.get_preprocess_shape" href="#sam_onnx.SamDecoder.get_preprocess_shape">get_preprocess_shape</a></code></li>
<li><code><a title="sam_onnx.SamDecoder.run" href="#sam_onnx.SamDecoder.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sam_onnx.SamEncoder" href="#sam_onnx.SamEncoder">SamEncoder</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>